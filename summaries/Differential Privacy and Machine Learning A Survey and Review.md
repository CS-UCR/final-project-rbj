# Differential Privacy and Machine Learning: a Survey and Review
Paper Link:\
https://arxiv.org/pdf/1412.7584.pdf

## Introduction
The underlying purpose of machine learning is to use existing data to determine existing trends that may be useful for later analysis of similar data, whether that analysis includes classifications, predictions, or clusters of similar data. Machine learning algorithms use various models and attempt to determine the most accurate model for future use. The models naturally contain information about the data which they are trained on, which conflicts with recent initiatives to preserve the privacy of individuals' data. Previous understandings of privacy believed anonymization to be sufficient in protecting individuals' privacy, but various instances have proven such assumptions false. Anonymization often falls apart if an attacker has previous information about a particular individual, and, thus, identifying that individual within other datasets becomes quite easy. Other approaches to privacy that provide stronger guarantees than anonymization have also been developed, but the attributes of these approaches still allow attackers to make certain inferences about their target dataset. When combined with previous knowledge of particular individuals, these inferences can be used to de-anonymize individuals, and, as a result, privacy breaches are quite possible. Such inference techniques also work on aggregate statistics. Learning from the weaknesses of these approaches to privacy, differential privacy provides a novel approach to privacy which directly addresses differences between different datasets and limits the inferences which can be made based on those differences. As such, differential privacy seeks to provide higher standards of privacy for individuals and their data.

## Differential Privacy
### Definition
The following definition formalizes the concept of $(\epsilon, \delta)$-privacy.
$$P(\tilde{f}(D) \in S) \leq e^{\epsilon}P(\tilde{f}(D') \in S) + \delta$$
Under this definition, D and D' are two datasets with a distance of 1, where the distance between datasets is the amount of entries in which they differ. Such datasets are also referred to as neighboring datasets. $\tilde{f}$ simply refers to a random function which takes a dataset as an input and returns a random variable as an output. This output can be a discrete or continuous value, and does not necessarily have to be numeric. Lastly $\epsilon$ and $\delta$ are the 2 privacy parameters which serve as the basis of the strength of privacy guarantees for differential privacy. Using this definition of privacy, privacy can be understood to be enforced by $\tilde{f}$ if the probabilities of the outputs of 2 neighboring datasets fall within the bounds established by $\epsilon$ and $\delta$. The simplified case of $(\epsilon, 0)$-differential privacy is referred to as $\epsilon$-differential privacy, and simply has the additional expectation that there is no risk of data leakage. Under the weaker definition where $\delta$ is greater than 0, $\delta$ still must be sufficiently small, typically less than 1/n, to ensure privacy.
### Mechanisms
#### Laplace Mechanism
The Laplace mechansism is a popular method of noise generatio which satisfies $\epsilon$-differential privacy for numeric data in relation to the sensitivity of neighboring datasets. The sensitivity of neighboring datasets is defined as:
$$s(f, \|.\|) = \max_{d(D,\ D')}\|f(D) - f(D')\|,$$
where f is a query on the datasets adnd $\|.\|$ is a norm function, typically $L_{1}$ or $L_{2}$. The Laplace mechanism, then, is a random function which simply adds noise, $\eta$, to the output of a query, where $\eta$ is drawn from the Laplace distribution with density $p(\eta) \propto e^{-\epsilon\|\eta\|/s(f, \|.\|)}$. The Gausian mechanism is a variation which satisfies $(\epsilon, \delta)$-differential privacy by generating noise from a Gaussian distribution instead. The weaker privacy guarantees are exchanged for less extreme noise, and, as such, maintains higher utility of a dataset.
#### Exponential Mechansism
While the Laplace and Gaussian mechanisms generate noise for numeric data, numeric data is not the only data which exists. The exponential mechanism provides a more general privacy mechansism by employing a scoring function to determine the output of a given query. The mechanism functions by scoring each possible output and assigns probabilities in proportion to each outputs score and the sensitivity of the scoring function. Natually this scoring function favors accurate answers, but the introduction of probability introduces noise which is difficult to distinguish from real query answers.
#### Smooth Sensitivity Framework and Sample-and-Aggregate Framework
In some cases, generating large noise significantly reduces the utility of a dataset, so reduced noise may be desirable. Smooth sensitivity provides a framework where $(\epsilon, \delta)$-differential privacy can be maintained by producing noise in accordance to the database as well as the query being passed. This framework allows for reduced noise by avoiding worst-case sensitivity by using the concepts of local and smooth sensitivities. The previous definition of sensitivity is typically referred to as global sensitivity. Local senstitivity is defined as:
$$LS(f, \|.\|, D) = \max_{D':d(D, D') = 1}\|f(D) - f(D')\|$$
One might think that generating noise in accordance to local sensitivity would reduce the noise sufficiently, however such noise risks being significantly reduced when compared to the noise generated using global sensitivity. In such cases where noise becomes significantly reduced, privacy guarantees become too weak since the noise does not sufficiently randomize the result of a query. The Smooth Sensitvity framework addresses the oversight of using local sensitivity by using smooth sensitivity to scale Gaussian noise so that it is sufficient to guarantee privacy. The Sample-and-Aggregate framework avoids extreme noise by partitioning a dataset into $k$, small subsets, then estimating the desired query for each such subset. If the desired query can be accurately estimated using small subsets, the query results for the subsets will retain enough accuracy to be useful, though these estimates do not yet have any noise. After estimating the desired query, the subset results are the aggregated. After aggregating the subsets, the smooth sensitivity framework is then applied to introduce noise, which ensures differential privacy.
### Combination of Mechanisms
Sometimes different mechanisms may be used in sequence to ensure differential privacy across a series of queries. In such cases, differential privacy is maintained for the entire sequence of queries, as long as each query is performed using a differentially-private mechanism. The sequential theorem states that the $(\epsilon, \delta)$ privacy guarantees of a series of differentially private queries are equal to the sums of the respective $(\epsilon, \delta)$ guarantees for each query. Naturally, this allows $\epsilon$ to be split across the series of queries to ensure the total privacy guarantees are in accordance to a desired $\epsilon_{total}$, rather than an $\epsilon$ which is higher than desired, and thus has weaker privacy. The parallel theorem also addresses sequences of differential private mechanisms in accordance to the subsets of a dataset. If a differentially private mechanism is applied to each subset of a dataset, then any function using the respective outputs of $n$ subsets of a dataset will also yield differentially private outputs. Put simply, combining differentially private outputs yields differentially private outputs.