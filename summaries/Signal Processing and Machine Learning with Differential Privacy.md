# Signal Processing and Machine Learning with Differential Privacy
Paper Link:\
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6582713&tag=1

## Introduction
Various models and definitions exist for performing computations while maintaining privacy, but may still be susceptible to certain forms of attack. Composition attacks are quite effective at reidentifying individuals even when certain privacy measures have been enacted. They are effective due to the use of prior-known information which is used to identify individuals within a dataset. Since data fundamentally represents the identity of individuals, protecting data is essential to protecting the privacy of individuals. Differential privacy is a cryptographically-inspired approach to privacy which ensures a certain risk level of identification is maintained for an individual between any two neighboring datasets. The risk level is often denoted as $\epsilon$, and low values imply lower risk of a breach of privacy. Differential privacy relies on features of statistics to maintain privacy and differs from privacy definitions based on principles from cryptography or information theory.\
Differential privacy focuses on ensuring privacy when data is retrieved thorugh queries. This is performed by approximating the result of a query rather than returning an exact answer, and this approach fundamentally relies on the introduction of noise into the dataset. Unfortunately, there is a fundamental tradeoff between the level of noise in a dataset and the utility of the dataset: as noise increases, privacy increases and utility decreases. Thus, advances in differential privacy focus on how to maximize privacy and utility simultaneously. The complexity of continous data compared to discrete data also means that differential privacy is easier to understand and implement for discrete data. As such, more study is required for how differential privacy might be applied to continous data, but such study might be able to learn form existing signal processing techniques.
## Definitions of Differential Privacy
Differential privacy introduces noise using randomization, meaning it is impossible to determine which values within a dataset have been modified without accessing the original dataset. As such, a formal mathematical definition of differential privacy is motivated by probability and can be understood through 2 primary definitions:\
$\epsilon$-differential privacy
$$P(A_{priv}(D)\in S) \leq e^{\epsilon} \cdot P(A_{priv}(D') \in S)$$
$(\epsilon, \delta)$-differential privacy
$$P(A_{priv}(D)\in S) \leq e^{\epsilon} \cdot P(A_{priv}(D') \in S) + \delta$$

In both definitions, D refers to an arbitrary dataset, while D' represents any neighboring dataset of D, or a dataset that differs from D by exactly 1 entry. From each of the definitions, an algorithm, $A_{priv}$ can be understood to be differentially private if the probability of its output for D is at most $e^{\epsilon}$ times as likely to occur as the output of $A_{priv}$ for D'. The second definition also includes $\delta$ which accounts for the probability of a data breach. In both defintions, privacy increases with low values of $\epsilon$ and $\delta$.

Since these definitions of differential privacy emphasize the output of a given algorithm, the results of the algorithm can be understood to follow differential privacy and containing any noise generated by the algorithm. As such, usage of such outputs in later computations will also be differentially private due to the inclusion of noise in those future computations. Since differential privacy relies on inserted noise  to maintain privacy, that privacy will be guaranteed as long as the original data is not used during postprocessing. Additionally, if 2 separate privacy algorithms are used for separate computations, the total risk will be at most $\epsilon_{1} + \epsilon_{2}$. Risk can be further reduced using $(\epsilon, \delta)$-differential privacy.

## Differential Privacy Methods
When implementing noise for the sake of privacy, said noise can be inserted at various points in the analysis process. The following are the 4 methods by which noise can be introduced during analysis.
### Input Perturbation
The simplest way of introducing noise is by adding noise directly to a dataset which will be used for inputs. Noise added in this way will be a vector with density $p_{z}(z) \propto \exp(-\frac{\epsilon}{2} \|z\|)$ and the same amount of dimensions as any given entry in a dataset. For entries with only 1 dimension, noise is added using the Laplace distribution. Other distributions can also be used for generate noise and may provide better utiltiy depending on the data.
### Output Perturbation
Another method by which noise can be introduced is by adding noise the outputs of an algorithm. In such cases, the generated noise relies on the sensitivity of any given function. The sensitivity of a function is defined as
$$S(f) = \max_{D \sim D'}\|f(D) - f(D')\|.$$

Similarly to input perturbation, the noise added is a vector that has a density $p_{z}(z) \propto \exp(-\frac{\epsilon}{S(f)} \|z\|)$. The amount of noise added using this method may make the output low utility, so variations may use local sensitivity to reduce the added noise.
### Exponential Mechanism
The exponential mechanism adds noise by assigning probabilities to possible outputs. The probabilities are assigned in such a way that high utility outputs are much more likely to occur, and as such can be applied in many cases. This method is particularly useful when data is non-discrete. The primary limitation is the ease by which a distribution can be sampled for noise. If a noise distribution is difficult to compute quickly, this method may take prohibitively long to execute.
### Objective Perturbation
In machine learning, optimization functions are used to determine classifiers for a given entry within a dataset. Objective perturbation generates noise by modifying such optmitization functions so that the "optimal" result is an approximation, and effectively only adds noise to entries near the edges of a given classifier. Noise is usually added according to a Laplace distribution, but Gaussian noise can also be used for $(\epsilon, \delta)$-differential privacy. In such cases where Gaussian noise is used, sensitivity parameters depend on the function being modified rather than the dataset. There is also further work being performed to study whether certain properties of data, such as incoherence, can be used to introduce less noise and improve performance.
## Differential Privacy in Statistics
The use of differential privacy is particularly obvious for statistical estimators such as mean, median, covariance matrices, etc. The differences between estimators can be used to illustrate the use of the perviously mentiond methods. For instance, differential privacy can be maintained when calculating a mean simply through input perturbation. Since the mean relies on every value for a specific variable, the noise of each value will contribute to the calculation. The resulting mean will be accurate to the aggregate data, but will not reveal new data if recalculated using a neighboring dataset since the amount of noise introduced will be unknown. Another example would be approximating a variable's median using differential privacy methods. Unlike calculating means, medians only depend on values close to the center of a variable's distribution. As a result, when different values occur at similar rates for a single variable, the inclusion or exclusion of a single data point may change the median, which would violate differential privacy. The solution is to use the exponential mechanism, so that values close to the actual median are highly likely. By using the exponential mechanism, drastic differences in the median of 2 neighboring datasets are significantly less likely, and thus differential privacy is maintained.
### Robust Statistics
As the previous examples have illustrated, different estimators should use different noise methods to ensure differential privacy. This is closely related to the field of robust statistics, which measures the properties of statistical estimators and how they are affected by noise or changes in the data. The previous examples demonstrate the difference between robust and non-robust estimators: robust estimators do not change much due to a single value, while non-robust estimators do. The mean is non-robust since the inclusion of an outlier drastically affects its value. In contrast, the median is robust since it is dependent on values close to the center of the distribution and relatively unaffected by outliers. Various researchers have realized the connections between differential privacy and robust statistics and have used those connections to create differentially private approximations of different estimators.
## Signal Processing and Machine Learning With Privacy
With the widespread use of machine learning and signal processing, research into privacy-preserving algorithms has increased. Algorithms exist for a wide variety of tasks, including classification, regression, principal components analysis, boosting, and online learning. The following subsections explain recent work in such algorithms.
### Classification and Regression
Due to the simplicity of discrete data, classification and regression are able to be performed using differentially private decision trees. However, for continous data, the typical method used is empirical risk minimization (ERM). In ERM, labeled data is used to find a vector which can be used classify an entry based on its features. Noise can be introduced to ERM using either output perturbation or objective perturbation. Objective perturbation follows a modified function which is used to determine the classification vector and generally has lower loss than output pertrubation. It additionally has performance which is similar to non-private algorithms, so it is not too costly to implement.
### Dimesionality Reduction
Dimensionality reduction aims to determine the dimensions, or subspace, associated with a phenomenon in data with high dimensionality. The usual algorithm, known as the PCA algorithm, generates a covariance matrix for the data in question and performs various transformations to determine the most relevant dimensions. A differentially private version of the algorithm known as sublinear queries adds noise to the generated covariance matrix before performing the usual transformations. Another proposed method involves sampling from the Bingham distribution, which would result in sampling noise close to the desired true subspace. However, the Bingham distribution is difficult to sample, and since differential privacy relies on accurate noise, poor sampling may prevent effective privacy.
### Time Series and Filtering
Recent work has also been performed in regard to time series and filtering. One recently proposed method involved generating noise for data within the Fourier domain and using homomorphic encryption to distribute noise. The use of homomorphic encryption means that the noise can be generated while the data is encrypted and be maintained after the data is decrypted. Additionally, several studies focused on the differences between input and output perturbation for signal aggregation and Kalman filtering. They found that input perturbation sometimes generates better noise due to Kalman filtering. Typically with machine learning, input perturbation creates too much noise, which prevents effective learning, but the cases with better noise due to filtering may likely have more effective learning.

## Issues and Limitations
Currently, much of differential privacy is theoretical, and as such, involves idealized scenarios based on particular assumptions. However, in the real world, such assumptions and scenarios are not guaranteed, and as a result, differential privacy may not be as effective as expected. In particular, the lack of study in regards to continuous data may be limit the practical limiations of the tasks which can be performed while maintaining differential privacy.
Another prominent issue is determining the privacy parameters $\epsilon$ and $\delta$. Although we know that smaller values of each parameter ensures greater privacy, determining the exact risks are difficult. There are also issues if computations are performed in sequence. In such scenarios, the $\epsilon$ chosen is typically for the entire sequence of computations and must be divided across said computations. In regards to $\delta$, the only agreement on how to choose $\delta$ is that it should be small and constant, typically much smaller than $\frac{1}{n^{2}}$. Since differential privacy relies on noise, larger sample sizes are required to maintain utility at any given value of $\epsilon$. In practice, this can be a very limiting factor as various datasets only contain a small number of entries. For such cases, the tradeoff for privacy often involves finding the lowest $\epsilon$ that allows the dataset to remain usable. Interestingly enough, there are also possible limitations due to current implementations of floating point arithmentic. These possible limitations might prevent complex systems from becoming differentially private unless $\epsilon$ is large.
